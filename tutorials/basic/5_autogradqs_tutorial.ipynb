{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddaa6e58",
   "metadata": {},
   "source": [
    "# Diferenciación Automática con ``torch.autograd``\n",
    "\n",
    "Al entrenar redes neuronales, el algoritmo más frecuentemente usado es\n",
    "**back propagation**. En este algoritmo, los parámetros (pesos del modelo) se\n",
    "ajustan de acuerdo al **gradiente** de la función de pérdida con respecto al\n",
    "parámetro dado.\n",
    "\n",
    "Para calcular esos gradientes, PyTorch tiene un motor de diferenciación automática\n",
    "integrado llamado ``torch.autograd``. Soporta el cálculo automático del gradiente para cualquier\n",
    "gráfico computacional.\n",
    "\n",
    "Considera la red neuronal más simple de una capa, con entrada ``x``, parámetros ``w`` y ``b``, \n",
    "y alguna función de pérdida. Puede ser definida en PyTorch de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e392f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.ones(5)  # tensor de entrada\n",
    "y = torch.zeros(3)  # salida esperada\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x, w)+b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed8a060",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(z)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebe556b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcccc5b0",
   "metadata": {},
   "source": [
    "En esta red, ``w`` y ``b`` son **parámetros**, los cuales necesitamos\n",
    "optimizar. Por lo tanto, necesitamos poder calcular los gradientes de la\n",
    "función de pérdida con respecto a esas variables. Para hacer eso, establecemos\n",
    "la propiedad ``requires_grad`` de esos tensores.\n",
    "\n",
    "> Puedes establecer el valor de ``requires_grad`` al crear un tensor, o después usando el método ``x.requires_grad_(True)``."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f839aa5c",
   "metadata": {},
   "source": [
    "Una función que aplicamos a los tensores para construir el gráfico computacional\n",
    "es de hecho un objeto de la clase ``Function``. Este objeto sabe cómo calcular la función\n",
    "en la dirección *hacia adelante*, y también cómo calcular su derivada durante el paso\n",
    "de *propagación hacia atrás*. Una referencia a la función de propagación hacia atrás se\n",
    "almacena en la propiedad ``grad_fn`` de un tensor. Puedes encontrar más información sobre ``Function`` en la [documentación](https://pytorch.org/docs/stable/autograd.html#function)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ec9e6b",
   "metadata": {},
   "source": [
    "Para optimizar los pesos de los parámetros en la red neuronal, necesitamos\n",
    "calcular las derivadas de nuestra función de pérdida con respecto a los parámetros,\n",
    "es decir, necesitamos $\\frac{\\partial loss}{\\partial w}$ y $\\frac{\\partial loss}{\\partial b}$\n",
    "bajo algunos valores fijos de ``x`` y ``y``. Para calcular esas derivadas, llamamos\n",
    "``loss.backward()``, y luego recuperamos los valores de ``w.grad`` y ``b.grad``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16eb3b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Gradient function for z = {z.grad_fn}\")\n",
    "print(f\"Gradient function for loss = {loss.grad_fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a25c26e",
   "metadata": {},
   "source": [
    "> **Nota**\n",
    "> - Solo podemos obtener las propiedades ``grad`` para los nodos hoja del gráfico computacional, que tienen la propiedad ``requires_grad`` establecida en ``True``. Para todos los otros nodos en nuestro gráfico, los gradientes no estarán disponibles.\n",
    "> - Solo podemos realizar cálculos de gradiente usando ``backward`` una vez en un gráfico dado por razones de rendimiento. Si necesitamos hacer varias llamadas ``backward`` en el mismo gráfico, necesitamos pasar ``retain_graph=True`` a la llamada ``backward``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f471fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcbfbde",
   "metadata": {},
   "source": [
    "## Deshabilitando el Seguimiento del Gradiente\n",
    "\n",
    "Por defecto, todos los tensores con ``requires_grad=True`` están siguiendo su\n",
    "historial computacional y soportan el cálculo del gradiente. Sin embargo, hay algunos casos donde no necesitamos\n",
    "hacer eso, por ejemplo, cuando hemos entrenado el modelo y solo queremos aplicarlo a\n",
    "algunos datos de entrada, es decir, solo queremos hacer cálculos *hacia adelante* a través de la red.\n",
    "Podemos detener el rastreo de los cálculos envolviendo nuestro código de cálculo con el bloque ``torch.no_grad()``:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ae2d00",
   "metadata": {},
   "source": [
    "Otra forma de lograr el mismo resultado es usando el método ``detach()`` en el tensor:\n",
    "\n",
    "El tensor resultante no tiene ``requires_grad=True``\n",
    "\n",
    "Hay razones por las que podrías querer deshabilitar el seguimiento del gradiente:\n",
    "  - Para marcar algunos parámetros en tu red neuronal como **parámetros congelados**.\n",
    "  - Para **acelerar los cálculos** cuando solo estás haciendo un paso hacia adelante, porque los cálculos en tensores que no rastrean gradientes serían más eficientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844a8cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53696636",
   "metadata": {},
   "source": [
    "Otra forma de lograr el mismo resultado es usando el método ``detach()`` en el tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9510be9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "z_det = z.detach()\n",
    "print(z_det.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9065fd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Más sobre el Gráfico Computacional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd83bb4f",
   "metadata": {},
   "source": [
    "Conceptualmente, autograd mantiene un registro de datos (tensores) y todas las operaciones ejecutadas\n",
    "(junto con los nuevos tensores resultantes) en un gráfico acíclico dirigido (DAG) que consiste de\n",
    "objetos [Function](https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function). En este DAG, las hojas\n",
    "son los tensores de entrada, las raíces son los tensores de salida. Rastreando este gráfico desde las raíces hasta las hojas, \n",
    "puedes calcular automáticamente los gradientes usando la regla de la cadena.\n",
    "\n",
    "En un paso hacia adelante, autograd hace dos cosas simultáneamente:\n",
    "\n",
    "- ejecuta la operación solicitada para calcular un tensor resultante\n",
    "- mantiene la *función de gradiente* de la operación en el DAG.\n",
    "\n",
    "El paso hacia atrás se inicia cuando se llama ``.backward()`` en la raíz del DAG.\n",
    "``autograd`` luego:\n",
    "\n",
    "- calcula los gradientes de cada ``.grad_fn``,\n",
    "- los acumula en el atributo ``.grad`` del tensor respectivo\n",
    "- usando la regla de la cadena, se propaga hasta los tensores hoja.\n",
    "\n",
    "> **Nota**\n",
    ">\n",
    "> **DAGs son dinámicos en PyTorch**\n",
    "> Una cosa importante a notar es que el gráfico se recrea desde cero; después de cada llamada a ``.backward()``, autograd comienza a poblar un nuevo gráfico. Esto es exactamente lo que te permite usar declaraciones de flujo de control en tu modelo; puedes cambiar la forma, tamaño y operaciones en cada iteración si es necesario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53140cc0",
   "metadata": {},
   "source": [
    "## Gradientes de Tensores y Funciones Jacobianas\n",
    "\n",
    "En muchos casos, tenemos una función escalar de pérdida, y necesitamos calcular el gradiente \n",
    "con respecto a algunos parámetros. Sin embargo, hay casos donde la función de salida\n",
    "es un tensor arbitrario. En este caso, PyTorch te permite calcular el llamado **producto Jacobiano-vector**, en lugar de la matriz Jacobiana actual.\n",
    "\n",
    "Para un vector función $\\vec{y}=f(\\vec{x})$, donde $\\vec{x}=\\langle x_1,\\ldots,x_n\\rangle$ y\n",
    "$\\vec{y}=\\langle y_1,\\ldots,y_m\\rangle$, un gradiente de $\\vec{y}$ con respecto a\n",
    "$\\vec{x}$ es dado por la **matriz Jacobiana**:\n",
    "\n",
    "$$J=\\left(\\begin{array}{ccc}\n",
    "   \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n",
    "   \\vdots & \\ddots & \\vdots\\\\\n",
    "   \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
    "   \\end{array}\\right)$$\n",
    "\n",
    "En lugar de calcular la matriz Jacobiana en sí misma, PyTorch te permite calcular el **producto Jacobiano-vector** $J^T \\cdot v$ para un vector dado $v$. Esto se logra\n",
    "llamando a ``backward`` con $v$ como argumento. El tamaño de $v$ debería ser el mismo que\n",
    "el tamaño del tensor original, con respecto al cual queremos calcular el producto:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b5207a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.eye(4, 5, requires_grad=True)\n",
    "out = (inp+1).pow(2).t()\n",
    "out.backward(torch.ones_like(out), retain_graph=True)\n",
    "print(f\"Primer llamado\\n{inp.grad}\")\n",
    "out.backward(torch.ones_like(out), retain_graph=True)\n",
    "print(f\"\\nSegundo llamado\\n{inp.grad}\")\n",
    "inp.grad.zero_()\n",
    "out.backward(torch.ones_like(out), retain_graph=True)\n",
    "print(f\"\\nLlamado después de poner los gradientes en cero\\n{inp.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad60b41f",
   "metadata": {},
   "source": [
    "Observa que cuando llamamos ``backward`` por segunda vez con los mismos argumentos, el valor de\n",
    "los gradientes es diferente. Esto sucede porque al hacer la propagación hacia atrás, PyTorch\n",
    "**acumula los gradientes**, es decir, el valor de los gradientes calculados se suma al atributo ``.grad`` \n",
    "de todos los nodos hoja del gráfico computacional. Si quieres calcular los gradientes apropiados, necesitas\n",
    "poner la propiedad ``.grad`` en cero antes. En el entrenamiento de la vida real esto es hecho para nosotros por el optimizador.\n",
    "\n",
    "> **Nota**\n",
    ">\n",
    "> Anteriormente estábamos llamando a la función ``backward()`` sin parámetros. Esto es esencialmente equivalente a \n",
    "> llamar ``backward(torch.tensor(1.0))``, lo cual es una forma útil de calcular los gradientes en el caso de \n",
    "> una función escalar, como la pérdida durante el entrenamiento de redes neuronales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66171493",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Lectura Adicional\n",
    "\n",
    "- [API de Autograd](https://pytorch.org/docs/stable/autograd.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tutorials",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e75d982e",
   "metadata": {},
   "source": [
    "# Conjuntos de Datos y DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c97e85d",
   "metadata": {},
   "source": [
    "El código para procesar muestras de datos puede volverse desordenado y difícil de mantener; idealmente queremos que nuestro código de conjunto de datos\n",
    "esté desacoplado de nuestro código de entrenamiento del modelo para mejor legibilidad y modularidad.\n",
    "PyTorch proporciona dos primitivas de datos: ``torch.utils.data.DataLoader`` y ``torch.utils.data.Dataset``\n",
    "que te permiten usar conjuntos de datos pre-cargados así como tus propios datos.\n",
    "``Dataset`` almacena las muestras y sus etiquetas correspondientes, y ``DataLoader`` envuelve un iterable alrededor\n",
    "del ``Dataset`` para habilitar acceso fácil a las muestras.\n",
    "\n",
    "Las bibliotecas de dominio de PyTorch proporcionan un número de conjuntos de datos pre-cargados (como FashionMNIST) que\n",
    "son subclases de ``torch.utils.data.Dataset`` e implementan funciones específicas para los datos particulares.\n",
    "Pueden usarse para prototipar y hacer benchmark de tu modelo. Puedes encontrarlos\n",
    "aquí: [Conjuntos de Datos de Imágenes](https://pytorch.org/vision/stable/datasets.html),\n",
    "[Conjuntos de Datos de Texto](https://pytorch.org/text/stable/datasets.html), y\n",
    "[Conjuntos de Datos de Audio](https://pytorch.org/audio/stable/datasets.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cf51c4",
   "metadata": {},
   "source": [
    "## Cargar un Conjunto de Datos\n",
    "\n",
    "Aquí hay un ejemplo de cómo cargar el conjunto de datos [Fashion-MNIST](https://research.zalando.com/project/fashion_mnist/fashion_mnist/) de TorchVision.\n",
    "Fashion-MNIST es un conjunto de datos de imágenes de artículos de Zalando que consiste en 60,000 ejemplos de entrenamiento y 10,000 ejemplos de prueba.\n",
    "Cada ejemplo comprende una imagen en escala de grises de 28×28 y una etiqueta asociada de una de 10 clases.\n",
    "\n",
    "Cargamos el [Conjunto de Datos FashionMNIST](https://pytorch.org/vision/stable/datasets.html#fashion-mnist) con los siguientes parámetros:\n",
    "- ``root`` es la ruta donde se almacenan los datos de entrenamiento/prueba,\n",
    "- ``train`` especifica conjunto de datos de entrenamiento o prueba,\n",
    "- ``download=True`` descarga los datos de internet si no están disponibles en ``root``.\n",
    "- ``transform`` y ``target_transform`` especifican las transformaciones de características y etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c887967",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44031843",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1bd867",
   "metadata": {},
   "source": [
    "## Iterar y Visualizar el Conjunto de Datos\n",
    "\n",
    "Podemos indexar ``Datasets`` manualmente como una lista: ``training_data[index]``.\n",
    "Usamos ``matplotlib`` para visualizar algunas muestras en nuestros datos de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43ef676",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_map = {\n",
    "    0: \"Camiseta\",\n",
    "    1: \"Pantalón\", \n",
    "    2: \"Suéter\",\n",
    "    3: \"Vestido\",\n",
    "    4: \"Abrigo\",\n",
    "    5: \"Sandalia\",\n",
    "    6: \"Camisa\",\n",
    "    7: \"Zapatilla\",\n",
    "    8: \"Bolso\",\n",
    "    9: \"Botín\",\n",
    "}\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img, label = training_data[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1478efdd",
   "metadata": {},
   "source": [
    "## Crear un Conjunto de Datos Personalizado para tus archivos\n",
    "\n",
    "Una clase Dataset personalizada debe implementar tres funciones: `__init__`, `__len__`, y `__getitem__`.\n",
    "Echa un vistazo a esta implementación; las imágenes de FashionMNIST están almacenadas\n",
    "en un directorio ``img_dir``, y sus etiquetas están almacenadas separadamente en un archivo CSV ``annotations_file``.\n",
    "\n",
    "En las siguientes secciones, desglosaremos qué está sucediendo en cada una de estas funciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74df21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import decode_image\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = decode_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317b5f49",
   "metadata": {},
   "source": [
    "### `__init__`\n",
    "\n",
    "La función __init__ se ejecuta una vez al instanciar el objeto Dataset. Inicializamos\n",
    "el directorio que contiene las imágenes, el archivo de anotaciones, y ambas transformaciones (cubiertas\n",
    "en más detalle en la siguiente sección).\n",
    "\n",
    "El archivo labels.csv se ve así:\n",
    "\n",
    "```\n",
    "tshirt1.jpg, 0\n",
    "tshirt2.jpg, 0\n",
    "......\n",
    "ankleboot999.jpg, 9\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a89134",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "    self.img_labels = pd.read_csv(annotations_file)\n",
    "    self.img_dir = img_dir\n",
    "    self.transform = transform\n",
    "    self.target_transform = target_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390e159b",
   "metadata": {},
   "source": [
    "### `__len__`\n",
    "\n",
    "La función __len__ devuelve el número de muestras en nuestro conjunto de datos.\n",
    "\n",
    "Ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0533f155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __len__(self):\n",
    "    return len(self.img_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ccdbff",
   "metadata": {},
   "source": [
    "### `__getitem__`\n",
    "\n",
    "La función __getitem__ carga y devuelve una muestra del conjunto de datos en el índice dado ``idx``.\n",
    "Basado en el índice, identifica la ubicación de la imagen en el disco, la convierte a un tensor usando ``read_image``, \n",
    "recupera la etiqueta correspondiente desde el archivo csv en ``self.img_labels``, llama a las funciones de transformación \n",
    "en ellos (si aplica), y devuelve la muestra del tensor y la etiqueta correspondiente en una tupla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a51c5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __getitem__(self, idx):\n",
    "    img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "    image = decode_image(img_path)\n",
    "    label = self.img_labels.iloc[idx, 1]\n",
    "    if self.transform:\n",
    "        image = self.transform(image)\n",
    "    if self.target_transform:\n",
    "        label = self.target_transform(label)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54537616",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Preparar tus datos para el entrenamiento con DataLoaders\n",
    "\n",
    "El ``Dataset`` recupera las características de nuestro conjunto de datos y etiqueta una muestra a la vez. Mientras entrenamos un modelo, típicamente queremos pasar muestras en \"minibatches\", reordenar los datos en cada época para reducir el overfitting del modelo, y usar el multiprocesamiento de Python para acelerar la recuperación de datos.\n",
    "\n",
    "``DataLoader`` es un iterable que abstrae esta complejidad para nosotros en una API fácil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2f08ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16b1c37",
   "metadata": {},
   "source": [
    "## Iterar a través del DataLoader\n",
    "\n",
    "Hemos cargado ese conjunto de datos en el ``DataLoader`` y podemos iterar a través del conjunto de datos según sea necesario.\n",
    "Cada iteración a continuación devuelve un lote de ``train_features`` y ``train_labels`` (conteniendo elementos de ``batch_size=64`` cada uno).\n",
    "Debido a que especificamos ``shuffle=True``, después de que iteremos sobre todos los lotes, los datos se mezclan (para un control más detallado sobre el orden de carga de los datos, echa un vistazo a [Samplers](https://pytorch.org/docs/stable/data.html#data-loading-order-and-sampler)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaac62aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar imagen y etiqueta.\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Forma del lote de características: {train_features.size()}\")\n",
    "print(f\"Forma del lote de etiquetas: {train_labels.size()}\")\n",
    "print(train_features[0].shape)\n",
    "img = train_features[0].squeeze()\n",
    "print(img.shape)\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Etiqueta: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc37788",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Lectura Adicional\n",
    "- [torch.utils.data API](https://pytorch.org/docs/stable/data.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f656a3f5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tutorials",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
